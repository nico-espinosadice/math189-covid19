{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 1: Analyzing COVID-19 Data with Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the coding part of this assignment we will conduct regression analysis on COVID-19 data to understand variables that affect COVID-19 growth rate. \n",
    "\n",
    "To complete this part of the assignment, follow along with the data loading and cleaning (running each cell as you go), looking up functions you are unfamiliar with and making sure you understand each step. While this part is not super fun, it is very important that you understand and are familiar with techniques for data manipulation so that you can use them later. When you arrive at the tasks, follow the instructions and then get started on your own research. \n",
    "\n",
    "Check out [this](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/) blog post for some Jupyter Notebook tips to get that work ~flowing~. Also, we use Pandas a lot in this assignment so if you are unfamiliar with this package and run into trouble, we recommend you check out a tutorial online. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "We got our data from Johns Hopkins Hopkins University. It gives us cumulative totals for confirmed cases, deaths, and recovered cases on the country level. The most up-to-date data can be found [here](https://data.humdata.org/dataset/novel-coronavirus-2019-ncov-cases) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_confirmed = pd.read_csv('time_series_covid19_confirmed_global.csv')\n",
    "raw_deaths = pd.read_csv('time_series_covid19_deaths_global.csv')\n",
    "raw_recovered = pd.read_csv('time_series_covid19_recovered_global.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the structure of the data (the other data tables have the same structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_confirmed.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the data\n",
    "\n",
    "confirmed = raw_confirmed.drop(['Lat','Long'], axis = 1)\n",
    "deaths = raw_deaths.drop(['Lat','Long'], axis = 1)\n",
    "recovered = raw_recovered.drop(['Lat','Long'], axis = 1)\n",
    "\n",
    "# Removing province information so we have consistent country-level resolution\n",
    "\n",
    "def set_country_res(df):\n",
    "\n",
    "    df_sans_provinces = df.drop('Province/State', axis=1)\n",
    "    df_sans_provinces = df_sans_provinces.groupby('Country/Region').sum()\n",
    "    \n",
    "    return df_sans_provinces\n",
    "\n",
    "confirmed = set_country_res(confirmed)\n",
    "deaths = set_country_res(deaths)\n",
    "recovered = set_country_res(recovered)\n",
    "\n",
    "confirmed.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's visualize our data! We do this to make sure that our data is loaded in properly and matches our expectations -- if the data doesn't match you expectation you either made a mistake or a discovery (both are worth your time to find out early). As you work with data, remember to visualize early and often as a sanity check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confirmed.columns = pd.to_datetime(confirmed.columns)\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.xticks(rotation = 45)\n",
    "plt.plot(confirmed.columns, confirmed.loc['US'], label = 'US')\n",
    "plt.plot(confirmed.columns, confirmed.loc['China'], label = 'China')\n",
    "plt.plot(confirmed.columns, confirmed.loc['Spain'], label = 'Spain')\n",
    "plt.plot(confirmed.columns, confirmed.loc['Italy'], label = 'Italy')\n",
    "plt.plot(confirmed.columns, confirmed.loc['Brazil'], label = 'Brazil')\n",
    "plt.legend()\n",
    "plt.title('Cases Over Time')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gathering explanatory variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment we are trying to investigate causal relationships and correlations between COVID-19 country data and possible population statistics that could affect infection and death rates. So now, we will show you how to load and join possible explanatory variables. \n",
    "\n",
    "It has been shown that older individuals are at [higher risk of death due to COVID-19](https://www.cdc.gov/coronavirus/2019-ncov/need-extra-precautions/older-adults.html?CDC_AA_refVal=https%3A%2F%2Fwww.cdc.gov%2Fcoronavirus%2F2019-ncov%2Fspecific-groups%2Fhigh-risk-complications%2Folder-adults.html) so we decided to investigate if the median age of a country is correlated with its death rate as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load median age data\n",
    "median_age = pd.read_csv('median_age.csv')\n",
    "median_age.drop(['Median Male','Median Female'], axis=1, inplace=True)\n",
    "median_age.rename(columns={'Median':'median_age', 'Place':'Country/Region'},inplace=True)\n",
    "median_age.set_index('Country/Region', drop = True, inplace=True)\n",
    "median_age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now calculate the death rate for each country. To do this we decided to use \n",
    "\n",
    "$death \\; rate = \\frac{deaths}{confirmed \\; cases}$\n",
    "\n",
    "because at the time of writing this assignment, the number of resolved cases was very low and likely underreported. If we had better data, a more accurate representation of the death rate would be to use resolved cases in the denominator (where $resolved \\; cases = recovered + deaths$)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate death rate for each country\n",
    "\n",
    "# Get most recent numbers for recovered and deaths (last column in the data table)\n",
    "total_confirmed = confirmed[confirmed.columns[-1]]\n",
    "total_deaths = deaths[deaths.columns[-1]]\n",
    "\n",
    "death_rate = pd.Series(dtype = float)\n",
    "\n",
    "# Calculating death rate\n",
    "if (total_deaths.index == total_confirmed.index).all():\n",
    "    death_rate = total_deaths/(total_confirmed + total_deaths)\n",
    "else:\n",
    "    print('Whoops, looks like your countries dont match')\n",
    "\n",
    "# Drop countries that have a null death rate (don't have any cases)\n",
    "death_rate.dropna(inplace=True)\n",
    "\n",
    "death_rate = pd.DataFrame(death_rate)\n",
    "death_rate.rename(columns={death_rate.columns[0]:'death_rate'},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A useful tool when working with multiple data tables is the merge function. This function merges two data tables (or columns from data tables) on a column or index of your choosing. One must be careful when merging because you can easily lose or multiply your data because of duplicate or mismatching keys. For this reason, it is important to always check the size of your new data table compared to the old ones. \n",
    "\n",
    "You can find more information about the pandas merge function [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.merge.html) and a more in depth explanation of merges [here](https://www.shanelynn.ie/merge-join-dataframes-python-pandas-index-1/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Merging death rate and median age \n",
    "naive_merge = median_age.merge(death_rate,left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We called our first merge a naive merge because we made the assumption that our two different data sources had all the same countries and named the countries the same. We will check that assumption in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding how many countries we lost in our naive merge\n",
    "print(f'We lost {death_rate.shape[0] - naive_merge.shape[0]} countries in our naive merge')\n",
    "\n",
    "# Finding the countries we lost\n",
    "right_merge = median_age.merge(death_rate,left_index=True, right_index=True, how='right')\n",
    "right_merge[right_merge.median_age.isnull()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the countries that we lost, we are going to add back the US and South Korea (which is called 'Korea, South' in the data set for some reason). To do this, we simply need to make the names match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To find the corresponding names in the median_age table, we guessed the obvious names \n",
    "# and they worked: 'United States' and 'South Korea'\n",
    "\n",
    "# But if you can't find a name you can try this\n",
    "# for country in median_age.index:\n",
    "#     print(country)\n",
    "\n",
    "# Now we will rename them in the median_age data (so it is consistent with the rest of our data)\n",
    "median_age.rename(index={'United States':'US','South Korea':'Korea, South'}, inplace=True)\n",
    "\n",
    "# Redoing our merge\n",
    "deaths_and_age = median_age.merge(death_rate,left_index=True, right_index=True)\n",
    "\n",
    "# Make sure it worked, we should have 2 fewer missing countries\n",
    "print(f'We lost {death_rate.shape[0] - deaths_and_age.shape[0]} countries in our merge')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now that we have workable data let's visualize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(deaths_and_age.median_age, deaths_and_age.death_rate, color='darkorange')\n",
    "plt.ylabel('Death Rate')\n",
    "plt.xlabel('Median Age')\n",
    "plt.title('Death Rate vs. Median Age')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Based on our initial scatter plot a linear relationship doesn't look super promising but let's try it anyway\n",
    "\n",
    "You can find more information about the package we used for the linear regression [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.linregress.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression using scipy\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(deaths_and_age.median_age, deaths_and_age.death_rate)\n",
    "predictions = np.linspace(15,55,5) * slope + intercept\n",
    "print(f'p-values: {p_value}')\n",
    "print(f'R^2: {r_value*r_value}')\n",
    "print(f'Slope: {slope}')\n",
    "\n",
    "# Plot Results\n",
    "plt.scatter(deaths_and_age.median_age, deaths_and_age.death_rate)\n",
    "plt.plot(np.linspace(15,55,5), predictions)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we predicted, our linear regression was not successful (as of the writing of this assignment). We encourage you to do your own research on methods of assessing regressions if you don't already have experience in this area and the grutors are happy to talk about it as well. Our regression above is likely an example of 'garbage in, garbage out'. This means that you can't get good information from bad data and coronavirus data is anything but consistent across countries, so you need to be careful about conclusions you draw from your analysis.\n",
    "\n",
    "Now we have to ask ourselves if we truly believe that age is not a factor in death rate or if our data is misrepresenting reality. Now this is where you come in. You may have noticed that some countries have extremely high death rates that are inconsistent with what we know about the virus. This is likely because the sample size is far too small and furthermore is biased towards critical cases due to a lack of testing. While we can't fix the testing problem, we can try to make the data more reliable by filtering based on sample size. \n",
    "\n",
    "Your first task is to filter the data based on a minimum sample size that you deem appropriate (we chose 1000 on March 28th) then re-run the regression and interpret the results. Don't worry if you don't get statistically significant/logical results. \n",
    "\n",
    "__Don't forget to save important graphs and statistics for your deliverable.__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Filter on Sample Size and Re-Run Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get an idea of the numbers of resolved cases for most countries\n",
    "total_confirmed.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Find Your Own Data\n",
    "Now, armed with the code above, you should pose a research question, find and download your own data from the internet, import it and run regressions (at least 2) to investigate your question. For each regression, write what you expect the relationship to be before you run the regression, graph the data, discuss how your results support/refute your initial hypothesis, and what you would need to be more sure of your results. \n",
    "\n",
    "You are welcome to continue working at a global scale, but there is also state by state data (and even county level data) in the United States which might yield more interesting results [here](https://covidtracking.com/?fbclid=IwAR3WwZ1nX8qhwJkAi1uYahgpyV94V3xPs0v_RzBBycMPB7p01DMKyDcc9Bk).\n",
    "\n",
    "There is a wealth of demographic data on the internet that you can couple with the COVID-19 data. You don't need to limit yourself to looking at the death rate, but you can look at any trends with respect to COVID-19 you find interesting.\n",
    "\n",
    "You can run linear regressions, transform your data (to fit an exponential or quadratic function, for example), run a Multiple Linear Regression, or anything else the questions you want to answer require. Feel free to use all internet resources (as long as you don't copy graphs and results), and the grutors are happy to answer any questions you have.\n",
    "\n",
    "### Make sure to save graphs and statistics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
